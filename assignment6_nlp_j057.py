# -*- coding: utf-8 -*-
"""Assignment6_NLP_J057.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BLbCfrMVB3OBmIjxlnjPIhZs8yNq8o4J
"""

# ================================
# ðŸ“Œ 1. SETUP
# ================================
!pip install -q sentence-transformers scikit-learn torch

import os
os.environ["WANDB_DISABLED"] = "true"  # âœ… disables Weights & Biases login

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from torch.utils.data import DataLoader
from sentence_transformers import SentenceTransformer, InputExample, losses, util
import matplotlib.pyplot as plt

# ================================
# ðŸ“Œ 2. LOAD + SPLIT DATA
# ================================
df = pd.read_csv("train.csv")

# âœ… Use a smaller subset for much faster training
df = df.sample(10000, random_state=42)

train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)
dev_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)

train_df.to_csv("quora_train.csv", index=False)
dev_df.to_csv("quora_dev.csv", index=False)
test_df.to_csv("quora_test.csv", index=False)

print(f"âœ… Saved splits: Train={len(train_df)}, Dev={len(dev_df)}, Test={len(test_df)}")

# ================================
# ðŸ“Œ 3. HELPER FUNCTION
# ================================
def evaluate_model(model, test_df):
    batch_size = 32
    q1_emb = model.encode(test_df['question1'].tolist(), batch_size=batch_size, convert_to_tensor=True)
    q2_emb = model.encode(test_df['question2'].tolist(), batch_size=batch_size, convert_to_tensor=True)
    cosine_scores = util.cos_sim(q1_emb, q2_emb)
    y_true = test_df['is_duplicate'].tolist()
    y_pred = [1 if score > 0.5 else 0 for score in cosine_scores.diag()]
    return f1_score(y_true, y_pred)

# ================================
# ðŸ“Œ 4. BI-ENCODER TRAINING
# ================================
train_samples = [InputExample(texts=[row['question1'], row['question2']], label=float(row['is_duplicate'])) for _, row in train_df.iterrows()]

def train_bi_encoder(model_name, loss_fn, epochs=1):
    model = SentenceTransformer(model_name)
    train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=32)
    model.fit(train_objectives=[(train_dataloader, loss_fn)], epochs=epochs, show_progress_bar=True)
    return model

bi_encoder_results = {}

# 1. Bi-Encoder with CosineSimilarityLoss
model_cs = train_bi_encoder('all-MiniLM-L6-v2', losses.CosineSimilarityLoss(SentenceTransformer('all-MiniLM-L6-v2')))
f1_cs = evaluate_model(model_cs, test_df)
bi_encoder_results["CosineSimilarityLoss"] = f1_cs

# 2. Bi-Encoder with ContrastiveLoss
model_cl = train_bi_encoder('all-MiniLM-L6-v2', losses.ContrastiveLoss(SentenceTransformer('all-MiniLM-L6-v2')))
f1_cl = evaluate_model(model_cl, test_df)
bi_encoder_results["ContrastiveLoss"] = f1_cl

# 3. Bi-Encoder with MultipleNegativesRankingLoss
model_mn = train_bi_encoder('all-MiniLM-L6-v2', losses.MultipleNegativesRankingLoss(SentenceTransformer('all-MiniLM-L6-v2')))
f1_mn = evaluate_model(model_mn, test_df)
bi_encoder_results["MultipleNegativesRankingLoss"] = f1_mn

# ================================
# ðŸ“Œ 5. RESULTS + BAR CHART
# ================================
print("\nðŸ“Š F1-SCORE RESULTS")
for model_name, score in bi_encoder_results.items():
    print(f"{model_name}: {score:.4f}")

plt.figure(figsize=(7,4))
plt.bar(bi_encoder_results.keys(), bi_encoder_results.values(), color='skyblue')
plt.ylabel("F1-Score")
plt.title("Bi-Encoder Performance on Quora Question Pairs")
plt.xticks(rotation=15)
plt.show()